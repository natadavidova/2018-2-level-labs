# Лабораторная работа №3

## Дано

> **Внимание**: текст на скачивание отличается от текста для лабораторной работы №2. 
> А именно, там убраны последние несколько тысяч строк, представляющие простое перечисление
> слов, не являющееся связным текстом.

1. Заданный английский текст большого размера.
Его нужно
[скачать](https://www.dropbox.com/s/0ww83i3tsupxfhz/not_so_big_reference_text.txt?dl=1)
и положить в папку `lab_3`. Таким образом, повторить структуру:
```
|-- 2018-2-level-labs
  |-- lab_3
    |-- not_so_big_reference_text.txt
```

> Дальнейшая работа происходит исключительно с заданным текстом, в отличие от предыдущих 
> лабораторных работ, где большой текст использовался в качестве источника данных, а анализ
> производился на заданном произвольном тексте. Для выполнения данной работы
> требуется использовать только заданный текст, доступный по ссылке. 

## Что надо сделать

### Шаг 0.1 Подготовка (проделать вместе с преподавателем на практике).

1. В имеющемся форке репозитория, обновить содержимое до последнего доступного
состояния в родительском репозитории.
2. Изменить файл `main.py`
3. Закоммитить изменения и создать pull request

### Шаг 0.2 Прочитать заданный текст из файла

Уже сделано, обратите внимание на содержимое файла `main.py` ближе к
концу:
```python
if __name__ == '__main__':
  with open('not_so_big_reference_text.txt', 'r') as f:
    REFERENCE_TEXT = f.read()
```

### Шаг 1. Разбиение текста на предложения и токенизация

Заданный английский текст необходимо разбить на предложения. Таким образом, получить
список из строк. В качестве разделителя использовать точку и факт того, что следующее слово
начинается с большой буквы. Все строки (предложения)
в нижнем регистре, не содержат никаких знаков пунктуации, в том числе и замыкающую
точку. 

Например, для текста `Mar#y wa$nted, to swim! However, she was afraid of sharks.` должен
получиться 

```json
[
  [
    "mary", "wanted", "to", "swim"
  ],
  [
    "however", "she", "was", "afraid", "of", "sharks"
  ]
]
```

> **Внимание**: предполагается массивное переиспользование логики из первых двух
лабораторных работ.

Внешний интерфейс выглядит так:

```python
def split_by_sentence(text: str) -> list:
  pass
```

### Шаг 2. Создание хранилища соответствий слово-число

Необходимо каждому слову из заданного текста присовить некоторый уникальный идентификатор
(id). Это требуется для того, чтобы работать не со строками напрямую, а с числами,
которые их представляют.

Например, есть слово "experience", поставим ему в соответствие некоторое уникальное 
число - 12345. Следующему слову "elimination" - 12346 и так далее. Выбор правила для присваивания 
идентификатора (счетчик, начинающийся с нуля или с заданного значения) - произвольный - на ваш
выбор. Основное условие - для одного и того же слова существует ровно один идентификатор. Одинаковых идентификаторов у двух разных слов также быть не может. Внешний интефейс такой: это класс `WordStorage`:

```python
class WordStorage:
  pass
```

### Шаг 2.1 Реализация метода наполнения хранилища новым словом

Для добавления слова в хранилище, реализуйте метод `put`, который принимает на вход новое слово
и возвращает его идентификатор.

```python
class WordStorage:
  ...
  def put(self, word:str) -> int:
    pass
```

### Шаг 2.2 Реализация метода получения идентификатора для уже имеющегося слова

Для любого слова можно попытаться получить его `id`. Для этого, реализуйте метод `get_id_of`, который
принимает на вход слово и возвращает его идентификатор. Если слово неизвестное, возвращается 
`None`.

```python
class WordStorage:
  ...
  def get_id_of(self, word:str) -> int:
    pass
```

### Шаг 2.3 Реализация метода получения оригинального слова по заданному идентификатору

Для любого `id` можно попытаться получить соответствующее ему слово. Для этого, реализуйте 
метод `get_original_by(id:int)`, который принимает на вход идентификатор и возвращает слово. 
Если идентификатор неизвестный, возвращается `None`.

```python
class WordStorage:
  ...
  def get_original_by(self, id:int) -> str:
    pass
```

### Шаг 2.4 Заполнение хранилища словами из корпуса (списка) предложений

Для этого воспользуемся полученным в результате Шага №1 списком предложений и
добавим все слова из него в экземпляр класса `WordStorage` с помощью метода
`from_corpus`. Готовый заполненный экземпляр будем активно использовать далее.

```python
class WordStorage:
  ...
  def from_corpus(self, id:tuple) -> str:
    pass
```

### Шаг 3. Кодирование (encoding) корпуса (списка) предложений

Полученный в результате Шага №1 корпус предложений необходимо кодировать с 
помощью заполненного экземпляра класса `WordStorage`. Кодирование
заключается в замене слов на соответствующие им идентификаторы.

Например, корпус из одно предложения:

```json
[
  [
    "experience", "elimination"
  ]
]
```

превращается в:

```json
[
  [
    12345, 12346
  ]
]
```

```python
def encode(storage_instance, corpus) -> list:
  pass
```

### Шаг 4. Создание структуры для хранения Н-грам (N-gram)

> Это не опечатка в названии класса. Такое название выбрано намеренно (`trie` является отраслевым
> термином).

Для успешного предсказания следующего слова на основании заданного контекста,
необходимо построить абстракцию, которая предоставляет достаточный для
этого интерфейс.

Класс `NGramTrie` позволяет собрать Н-грамы из заданного предложения.

Пример №1. Дано предложение `Mary wanted to swim`. Хотим построить `NGramTrie` с размером
грамы равным 2.  Тогда получаем следующие би-грамы:
`('<s>', 'mary')`, `('mary', 'wanted')`, `('wanted', 'to')`, `('to', 'swim')`, `('swim', '</s'>)`.

Пример №2. Дано предложение `Mary wanted to swim`. Хотим построить `NGramTrie` с размером
грамы равным 2.  Тогда получаем следующие три-грамы:`('<s>', 'mary', 'wanted')`, `('mary', 'wanted', 'to')`, `('wanted', 'to', 'swim')`, `('to', 'swim', '</s'>)`

> Символы`<s>, </s>` обозначают конец и начало предложения. Эти символы должны оказаться частью
> хранилища слов как самостоятельные слова. Попробуйте догадаться сами, зачем это необходимо и
> как это правильно сделать.

### Шаг 4.1 Объявление сущности языковой модели

Создадим класс:

```python
class NGramTrie:
  pass
```

Конструктор должен принимать на вход размер контекста ("N" из названия N-gram - 1 для уни-грам,
2 - для би-грам, 3 - для три-грам и т.д.). Это значение сохранется
в собственном поле экземпляра класса - `size`.

Сами Н-грамы должны храниться в собственном поле экземпляра класса - `gram_frequencies`.
Обязательно использовать для хранения словарь, где ключами выступают кортежи из чисел, а
значения - частота возникновения соответствующего кортежа в тексте.

Например, пусть есть некоторая би-грама `(1,2)`, которая встречалась в тексте 10 раз. Тогда
в классе она будет храниться так:

```python
self.gram_frequencies[(1,2)] = 10
```

Аналогично в классе хранятся и лог-вероятности, но в поле `gram_log_probabilities`. В одном
из следующих шагов разберемся как его правильно заполнять.

### Шаг 4.2 Реализация метода заполения Н-грам из заданного предложения

Цель данного метода заполнить внутреннее содержимое класса - `gram_frequencies`,
определенное в Шаге №4.

Заполнение происходит с помощью метода:

```python
class NGramTrie:
  ...
  def fill_from_sentence(self, sentence: tuple) -> str:
    pass
```

Метод `fill_from_sentence` возвращает метод код в виде строки: `'OK'` - если все в порядке. `'ERROR'` - если произошла ошибка.

> Напомним, что на самом деле на данном этапе вы уже работаете с закодироваными
> предложениями, полученными на Шаге №3. Текстовое описание дано для наглядности.

### Шаг 4.3 Расчет лог-вероятностей для каждой Н-грамы

Для расчета би-грам воспользуемся следующей формулой: 

<img src="https://latex.codecogs.com/gif.latex?P(w_{n}|w_{n-1})&space;=&space;\frac{C(w_{n-1},w_{n})}{\sum_{w}C(w_{n-1},&space;w)}" title="P(w_{n}|w_{n-1}) = \frac{C(w_{n-1},w_{n})}{\sum_{w}C(w_{n-1}, w)}" />

<img src="https://latex.codecogs.com/gif.latex?C(w_{n}|w_{n-1})" title="C(w_{n}|w_{n-1})" /> - это количество появлений кортежа 
<img src="https://latex.codecogs.com/gif.latex?(w_{n-1},&space;w_{n})" title="(w_{n-1}, w_{n})" /> 
в заданном тексте (частота).
<img src="https://latex.codecogs.com/gif.latex?\sum_{w}{C(w_{n-1},&space;w)}" title="\sum_{w}{C(w_{n-1}, w)}" /> - это 
количество появлений кортежей вида 
<img src="https://latex.codecogs.com/gif.latex?(w_{n-1}&space;w)" title="(w_{n-1} w)" />
для всех `w` в заданном корпусе.

Идея проста: насколько часто, би-грама, начинающаяся со слова
<img src="https://latex.codecogs.com/gif.latex?w_{n-1}" title="w_{n-1}" />
будет заканчиваться интересующим
нам словом 
<img src="https://latex.codecogs.com/gif.latex?w_{n}" title="w_{n}" />.

> Выполнение би-грам обязательно для всех студентов.

> Для студентов, желающих получить оценку 8 требуется реализовать три-грамы.

> Реализация N-грам требуется только для студентов, желающих получить оценку 9 или 10.
> Больше информации в Шагах №6 и №7.

После получения относительных вероятностей (описаны выше), необходимо взять логарифим по 
основанию `e` (натуральный логарифм)
от полученного отношения. В рамках текущей работы, это просто требование - оперировать лог-вероятностями.

Все значения вероятностей храним в словаре, аналогичном `gram_frequencies`, он должен называться
`gram_log_probabilities`.

Интерфейс такой:

```python
class NGramTrie:
  ...
  def calculate_log_probabilities(self):
  pass
```

### Шаг 4.4 Реализация предсказания предложения по заданному префиксу

Есть некоторый префикс, выраженный в виде списка закодированных слов. По нему мы хотим
сгенерировать целое предложение.

Интерфейс данного метода следующий:

```python
class NGramTrie:
  ...
  def predict_next_sentence(self, prefix: tuple) -> list:
    pass
```

В результате получаем список из закодированных слов, включая и сам префикс.

> Внимание: пользователь интерфейса не знает про существование специальных симоволов
> `<s></s>`, поэтому указывается префикс без них. Также и возвращаемое значение должно
> приходить без этих символов.

### Шаг 5. Генерация текста по заданному слову (набор предложений)

Требуется сгенерировать заданное количество предложений на основе языковой модели,
которая была построена на Шаге №4. В качестве критерия для выбора следующего слова
выступает лог-вероятности (больше - лучше).

В результате должен получиться корпус, аналогичный по структуре тому, что был создан на
Шаге №3.

Интерфейс такой:

```python
def generate_text(n_gram_model: NGramTrie, number_sentences: int, prefix: tuple) -> list:
  pass
```

Так как для генерации текста необходимо иметь отправную точку, например, для би-грам это
первое слово, для три-грам - первые два слова, то мы передаем этот префикс - список
из слов (строк) в эту функцию.

> Внимание: пользователь интерфейса не знает про существование специальных симоволов
> `<s></s>`, поэтому указывается префикс без них.

Если требуется сгенерировать несколько предложений (`N>1`), то в качестве префикса для второго
и следующих следует использовать требуемое количество слов с конца предыдущего предложения.
Для би-грам берётся последнее слово, для три-грам берутся последние два слова и так далее.
Это необходимо для сохранения связности получаемого текста.

### Шаг 6. Обобщение реализации языковой модели по Н-грам (N-gram)

> Обощение до Н-грам обязательно для претендентов на оценку 9.

В Шаге 4.3 было продемострировано как посчитать лог-вероятность для би-грам.

Формула для расчета вероятности для N-грам:

<img src="https://latex.codecogs.com/gif.latex?P(w_{n}|w_{n-1},w_{n-2},...,w_{n-N&plus;1})&space;=&space;\frac{C(w_{n-N&plus;1},&space;...,&space;w_{n-1},&space;w_{n})}{\sum_{w}C(w_{n-N&plus;1},&space;...,&space;w_{n-1},&space;w)}" title="P(w_{n}|w_{n-1},w_{n-2},...,w_{n-N+1}) = \frac{C(w_{n-N+1}, ..., w_{n-1}, w_{n})}{\sum_{w}C(w_{n-N+1}, ..., w_{n-1}, w)}" />

Не забываем про необходимость взятия натурального логарифма от полученной вероятности.

### Шаг 7. Реализация алгоритма многоуровной Н-грам (N-gram) модели

В общем случае, возникает ситуация, когда достаточно большой префикс неизвестен языковой
модели и, согласно определенным выше правилам обработки таких ситуаций, прекращаем генерацию
предложения. 

Например, дано предложение `Mary very wanted to swim in the pool` и текущая модель
работает на 8-граммах. Тогда мы используем префикс 
`('mary', 'very', 'wanted', 'to', 'swim', 'in', 'the')` и пытаемся найти все
Н-грамы с таким префиском. Вдруг, оказывается, что такого префикса нет. 

Очевидно, что
выходить с из такого поиска при отсутствии большой Н-грамы слишком рано и мы все еще
можем предложить более-менее адекватное продолжение текста. 

Для решения такой задачи существует большое количество алгоритмов, предлагается использовать 
стратегию выбора, навеянную алгоритмом  `Stupid Backoff`. Не смотрите на название, это 
эффективная и очень простая и часто используемая в задачах такого рода идея.

Продолжая пример, попробуем найти кандидата по грамам размера N-1, в нашем случае 7-грамы.
Если в модели есть такие грамы, то мы получаем кандидата, если нет, продолжаем понижать
размерность грам до тех пор, пока не встречаемся с би-грамами. Если нет и такой би-грамы (что 
весьма маловероятно), прекращаем работу.

Ссылки:

* [Полезный ресурс для пытливых умов](https://web.stanford.edu/~jurafsky/slp3/3.pdf)
* [Что такое лог-вероятности и зачем они нужны](https://en.wikipedia.org/wiki/Log_probability).
